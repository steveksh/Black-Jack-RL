{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "# utils\n",
    "action_mapper = {0: \"Stick\", 1: \"Hit\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "# initializing a dictionary to hold the Q-values for each state-action pair\n",
    "q_values = defaultdict(lambda: np.zeros(2))\n",
    "\n",
    "# one forward pass \n",
    "obs, info = env.reset() # initialize the environment\n",
    "done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-greedy algorithm\n",
    "With probability ε: choose a random action a ∈ A  \n",
    "With probability 1 − ε: choose a = argmax<sub>a</sub> Q(s, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Cumulative Sum 17 | Dealer current sum 3\n",
      "User chose to... Stick ...very greedily..\n"
     ]
    }
   ],
   "source": [
    "# how greedy 5% \n",
    "e = 0.5\n",
    "\n",
    "# e-greedy algorithm \n",
    "def e_greedy(obs, q_values, e=e):\n",
    "    if np.random.random() < e:\n",
    "        return random.choice([0,1])\n",
    "    else:\n",
    "        return int(q_values[obs].argmax())\n",
    "    \n",
    "action = e_greedy(obs, q_values)\n",
    "user_hand = env.unwrapped.player\n",
    "\n",
    "print('User Cumulative Sum', obs[0], '| Dealer current sum', obs[1])\n",
    "print('User chose to...', action_mapper.get(action), '...very greedily..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Cumulative Sum 17 | Dealer current hand 3\n",
      "User hand [7, 10]\n"
     ]
    }
   ],
   "source": [
    "# 1 episode\n",
    "\n",
    "\"\"\"\n",
    "next_obs: (user_sum, dealer_sum, usable_ace)\n",
    "reward: -1 or 0 or 1\n",
    "termination: True or False | if the episode is done like game over\n",
    "truncated: True or False | if the episode is truncated like time limit reached\n",
    "info: dict | additional information about the environment \n",
    "\"\"\"\n",
    "next_obs, reward, termination, truncated, info = env.step(action) # take action 0 (stick)\n",
    "\n",
    "print('User Cumulative Sum', next_obs[0], '| Dealer current hand', next_obs[1])\n",
    "print('User hand', env.unwrapped.player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rule in Q-Learning \n",
    "\n",
    "Q(S<sub>t</sub>, A<sub>t</sub>) ← Q(S<sub>t</sub>, A<sub>t</sub>) + α(R<sub>t+1</sub> + γ ⋅ Q(S<sub>t+1</sub>, A′) − Q(S<sub>t</sub>, A<sub>t</sub>))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Q(S<sub>t+1</sub>, A′)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game is over\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if termination:\n",
    "    print('Game is over')\n",
    "    future_q_value = 0\n",
    "\n",
    "else:\n",
    "    print('Game is not over. Future Q value is used to update Q')\n",
    "    future_q_value = np.max(q_values[next_obs])\n",
    "\n",
    "discount_factor = 0.1\n",
    "\n",
    "# td difference \n",
    "td_difference = reward + discount_factor*future_q_value - q_values[obs][action]\n",
    "td_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update the q table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "q_values[obs][action] += learning_rate * td_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {(17, 3, 0): array([0.001, 0.   ])})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
