{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym \n",
    "import numpy as np \n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# distribution functions \n",
    "from torch.distributions import Categorical \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Avg Return: -7.20\n",
      "Epoch 100 | Avg Return: -2.82\n",
      "Epoch 200 | Avg Return: -1.41\n",
      "Epoch 300 | Avg Return: -2.19\n",
      "Epoch 400 | Avg Return: -3.85\n",
      "Epoch 500 | Avg Return: -4.12\n",
      "Epoch 600 | Avg Return: -2.88\n",
      "Epoch 700 | Avg Return: -3.33\n",
      "Epoch 800 | Avg Return: -3.84\n",
      "Epoch 900 | Avg Return: -4.86\n",
      "Epoch 1000 | Avg Return: 1.33\n",
      "Epoch 1100 | Avg Return: -1.86\n",
      "Epoch 1200 | Avg Return: -0.41\n",
      "Epoch 1300 | Avg Return: -4.18\n",
      "Epoch 1400 | Avg Return: -2.11\n",
      "Epoch 1500 | Avg Return: 0.01\n",
      "Epoch 1600 | Avg Return: -3.26\n",
      "Epoch 1700 | Avg Return: -4.57\n",
      "Epoch 1800 | Avg Return: -1.98\n",
      "Epoch 1900 | Avg Return: -2.50\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Blackjack-v1\")\n",
    "policy = PPOPolicy(3, 2)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "clip_eps = 0.2\n",
    "gamma = 0.99\n",
    "epochs = 100_000\n",
    "batch_size = 32\n",
    "ppo_epochs = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    states, actions, log_probs, rewards, dones, values = [], [], [], [], [], []\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "            action_probs, state_value = policy(state_tensor)\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_obs, reward, done, _, _ = env.step(action.item())\n",
    "            reward *= 10\n",
    "\n",
    "            states.append(state_tensor)\n",
    "            actions.append(action)\n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            \n",
    "            # Detach value to prevent graph reuse error\n",
    "            values.append(state_value.squeeze().detach())\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "    # Compute returns and advantages\n",
    "    returns = []\n",
    "    discounted_sum = 0\n",
    "    for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "        if d: discounted_sum = 0\n",
    "        discounted_sum = r + gamma * discounted_sum\n",
    "        returns.insert(0, discounted_sum)\n",
    "\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    values = torch.stack(values)\n",
    "    advantages = returns - values  # values already detached above\n",
    "\n",
    "    # Convert to tensors\n",
    "    states = torch.cat(states)\n",
    "    actions = torch.tensor(actions)\n",
    "    old_log_probs = torch.stack(log_probs).detach()\n",
    "\n",
    "    # PPO update\n",
    "    for _ in range(ppo_epochs):\n",
    "        action_probs, state_values = policy(states)\n",
    "        dist = Categorical(action_probs)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        ratio = (new_log_probs - old_log_probs).exp()\n",
    "\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages\n",
    "\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = nn.functional.mse_loss(state_values.squeeze(), returns)\n",
    "\n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        avg_return = returns.mean().item()\n",
    "        print(f\"Epoch {epoch} | Avg Return: {avg_return:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
