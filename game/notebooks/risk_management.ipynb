{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q Learning with an Alternative Game Environment. \n",
    "Users are able to bet money now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "import random\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=4, output_dim=2):  # 2 actions: hit or stick\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fnn(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class BettingBlackjackEnv:\n",
    "    def __init__(self, base_env, starting_money=100):\n",
    "        self.env = base_env\n",
    "        self.starting_money = starting_money\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.money = self.starting_money\n",
    "        self.current_bet = 5  # default\n",
    "        obs, info = self.env.reset()\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, termination, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Scale reward by current bet\n",
    "        scaled_reward = reward * self.current_bet\n",
    "        self.money += scaled_reward\n",
    "\n",
    "        return obs, scaled_reward, termination, truncated, info\n",
    "\n",
    "    def place_bet(self, bet_amount):\n",
    "        self.current_bet = int(np.clip(bet_amount, 5, min(100, self.money)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def choose_bet(agent_money):\n",
    "    return random.choice([5, 10, 20, 50, 100]) if agent_money >= 5 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.bet_options = [5, 10, 20, 50, 100]\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Q-network for playing (hit/stand)\n",
    "        self.q_net = QNetwork(3, 2).to(self.device)\n",
    "\n",
    "        # Q-network for betting (predict best bet size)\n",
    "        self.bet_q_net = QNetwork(3, len(self.bet_options)).to(self.device)\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            list(self.q_net.parameters()) + list(self.bet_q_net.parameters()),\n",
    "            lr=self.params['lr']\n",
    "        )\n",
    "\n",
    "    def tensor(self, obs):\n",
    "        return torch.FloatTensor(obs).to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_action(self, obs):\n",
    "        \"\"\" Epsilon-greedy policy for action selection (hit/stand) \"\"\"\n",
    "        if np.random.random() < self.params['e']:\n",
    "            return random.choice([0, 1])\n",
    "        else:\n",
    "            obs_tensor = self.tensor(obs)\n",
    "            q_values = self.q_net(obs_tensor)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_bet(self, obs):\n",
    "        \"\"\" Epsilon-greedy policy for bet selection \"\"\"\n",
    "        if np.random.random() < self.params['e']:\n",
    "            return random.choice(self.bet_options)\n",
    "        else:\n",
    "            obs_tensor = self.tensor(obs)\n",
    "            q_values = self.bet_q_net(obs_tensor)\n",
    "            best_idx = torch.argmax(q_values).item()\n",
    "            return self.bet_options[best_idx]\n",
    "\n",
    "    def decay(self):\n",
    "        self.params['e'] = max(0.01, self.params['e'] - 0.001)\n",
    "\n",
    "    def update(self, current_state, next_state, reward, action, termination, bet):\n",
    "        \"\"\" Update both Q-networks (play and bet) \"\"\"\n",
    "        state_tensor = self.tensor(current_state)\n",
    "        q_values = self.q_net(state_tensor)\n",
    "        current_q = q_values[action]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_tensor = self.tensor(next_state)\n",
    "            next_q = torch.max(self.q_net(next_tensor))\n",
    "            td_target = reward + self.params['discount'] * next_q * (1 - int(termination))\n",
    "\n",
    "        loss = self.criterion(current_q, td_target)\n",
    "\n",
    "        # Update bet Q-network\n",
    "        bet_q_values = self.bet_q_net(state_tensor)\n",
    "        bet_index = self.bet_options.index(bet)\n",
    "        current_bet_q = bet_q_values[bet_index]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Use same reward as feedback for bet learning\n",
    "            bet_td_target = reward\n",
    "\n",
    "        bet_loss = self.criterion(current_bet_q, torch.tensor(bet_td_target).to(self.device))\n",
    "\n",
    "        total_loss = loss + bet_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.decay()\n",
    "\n",
    "        return total_loss.item()\n",
    "\n",
    "    def generate_q_table(self, usable_ace=False):\n",
    "        \"\"\"Generate a tabular Q-value dict for hit/stand policy\"\"\"\n",
    "        self.q_values = {}\n",
    "        for player_sum in range(12, 22):\n",
    "            for dealer_card in range(1, 11):\n",
    "                state = (player_sum, dealer_card, usable_ace)\n",
    "                state_tensor = self.tensor(state)\n",
    "                with torch.no_grad():\n",
    "                    q_vals = self.q_net(state_tensor)\n",
    "                self.q_values[state] = q_vals.cpu().numpy()\n",
    "\n",
    "    def generate_bet_table(self, usable_ace=False):\n",
    "        \"\"\"Generate a betting policy dict based on learned Q-values\"\"\"\n",
    "        self.bet_values = {}\n",
    "        for player_sum in range(12, 22):\n",
    "            for dealer_card in range(1, 11):\n",
    "                state = (player_sum, dealer_card, usable_ace)\n",
    "                obs = self.tensor(state)\n",
    "                with torch.no_grad():\n",
    "                    q_vals = self.bet_q_net(obs)\n",
    "                best_idx = torch.argmax(q_vals).item()\n",
    "                best_bet = self.bet_options[best_idx]\n",
    "                self.bet_values[state] = best_bet\n",
    "\n",
    "    def get_bet_grid(self, usable_ace=False):\n",
    "        \"\"\"Return a NumPy grid (dealer on Y, player sum on X) of best bets\"\"\"\n",
    "        self.generate_bet_table(usable_ace)\n",
    "        grid = np.zeros((10, 10))\n",
    "        for i, dealer_card in enumerate(range(1, 11)):\n",
    "            for j, player_sum in enumerate(range(12, 22)):\n",
    "                state = (player_sum, dealer_card, usable_ace)\n",
    "                grid[i, j] = self.bet_values.get(state, 0)\n",
    "        return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'discount': 0.6, 'lr': 0.001, 'e': 0.05}\n",
    "agent = Agent(params)\n",
    "losses = []\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = BettingBlackjackEnv(base_env=env)  # wrap original env\n",
    "\n",
    "episode_rewards = []\n",
    "episode_bets = []\n",
    "agent_money = []\n",
    "\n",
    "for episode in range(20_000):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # Agent chooses bet BEFORE seeing cards\n",
    "    bet = agent.greedy_bet([0, 0, 0])  # dummy input since no cards yet\n",
    "    env.place_bet(bet)\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.greedy_action(obs)\n",
    "        next_obs, reward, termination, truncated, info = env.step(action)\n",
    "\n",
    "        done = termination or truncated\n",
    "\n",
    "        # Update both Q-networks (play and bet)\n",
    "        loss = agent.update(obs, next_obs, reward, action, termination, bet)\n",
    "\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "\n",
    "    # Log this episode\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_bets.append(bet)\n",
    "    agent_money.append(env.money)  # total money after episode\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
